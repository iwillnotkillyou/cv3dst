{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04\n",
    "## Supervised Segmentations\n",
    "\n",
    "In this exercise, we will explore supervised segmentation for assigning semantic labels to individual pixels in the image. Knowledge about the semantics of the image is essential for image and video understanding and is crucial for technologies like autonomous driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "This will setup your whole environment such that you can work with the rest of the notebook.\n",
    "\n",
    "### General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:00:13.134087900Z",
     "start_time": "2024-01-16T17:00:08.081549100Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up directory paths and (optionally) mount in Google Colab\n",
    "If you work with google colab set the `USING_COLAB` variable to `True` and following cell to mount your gdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:00:13.229439700Z",
     "start_time": "2024-01-16T17:00:13.138087700Z"
    }
   },
   "outputs": [],
   "source": [
    "USING_COLAB = False\n",
    "USE_CPU = True\n",
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"cv3dst\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_03) is given.\n",
    "\n",
    "if USING_COLAB:\n",
    "    from google.colab import drive\n",
    "    import os\n",
    "\n",
    "    gdrive_path='/content/gdrive/MyDrive/cv3dst/exercise_04'\n",
    "\n",
    "    # This will mount your google drive under 'MyDrive'\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    # In order to access the files in this notebook we have to navigate to the correct folder\n",
    "    os.chdir(gdrive_path)\n",
    "    # Check manually if all files are present\n",
    "    print(sorted(os.listdir()))\n",
    "    root_dir = Path(gdrive_path).parent\n",
    "else:\n",
    "    # root_dir = Path('./cv3dst/')\n",
    "    root_dir = Path('/storage/remote/adm9/cv3dst/cv3ws23/cv3_exercises/cv3dst')\n",
    "dataset_dir = root_dir.joinpath(\"datasets\")\n",
    "output_dir = root_dir.joinpath('exercise_04', 'models')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() and not USE_CPU else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:00:21.734390800Z",
     "start_time": "2024-01-16T17:00:13.226306900Z"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.obj_segmentation import (\n",
    "    LinearProbingNet,\n",
    "    PACNet,\n",
    "    training,\n",
    "    visualize_model\n",
    ")\n",
    "from exercise_code import visualize_davis, load_davis_dataset, create_embeddings, downsample_annotations\n",
    "from exercise_code.test import (\n",
    "    test_linear_probing,\n",
    "    test_pix_adaptive_conv_nets,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Embeddings from Images\n",
    "\n",
    "The following lines of code will extract per-patch feature embeddings from the images of the Davis dataset. As this process will take a while, we recommend you carefully read through and familiarize yourself with the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Create Embeddings for the Images</h3>\n",
    "    <p> You only need to do this once. If you have successfully run the code below, set the <code>INITIAL_RUN</code> to <code>False</code> to skip this in the subsequent runs of the code for a speedup. After running the cell below, you should see image embeddings for the Davis dataset under <code>datasets/obj_seg/train/embeddings</code> and <code>datasets/obj_seg/test/embeddings</code>, respectively.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:00:45.443319200Z",
     "start_time": "2024-01-16T17:00:19.939338100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to C:\\Users\\pavli/.cache\\torch\\hub\\main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to C:\\Users\\pavli/.cache\\torch\\hub\\checkpoints\\dino_deitsmall16_pretrain.pth\n",
      "100%|██████████| 82.7M/82.7M [00:14<00:00, 5.95MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embddings for the training set\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '\\\\storage\\\\remote\\\\adm9\\\\cv3dst\\\\cv3ws23\\\\cv3_exercises\\\\cv3dst\\\\datasets\\\\obj_seg\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m dino\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating embddings for the training set\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 11\u001B[0m create_embeddings(dataset_dir\u001B[38;5;241m.\u001B[39mjoinpath(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobj_seg\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m), dino, device)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownsampling annotations for the training set\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     13\u001B[0m downsample_annotations(dataset_dir\u001B[38;5;241m.\u001B[39mjoinpath(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobj_seg\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m), PATCH_SIZE)\n",
      "File \u001B[1;32mC:\\school\\School\\cv3\\cv3dst\\exercise_04\\exercise_code\\data\\seg_datasets\\davis_obj_seg.py:95\u001B[0m, in \u001B[0;36mcreate_embeddings\u001B[1;34m(path, dino_model, device)\u001B[0m\n\u001B[0;32m     91\u001B[0m     feat_out[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mqkv\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m output\n\u001B[0;32m     93\u001B[0m dino_model\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblocks\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattn\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mqkv\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mregister_forward_hook(hook_fn_forward_qkv)\n\u001B[1;32m---> 95\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m category_dir \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(path\u001B[38;5;241m.\u001B[39miterdir()):\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (category_dir\u001B[38;5;241m.\u001B[39mis_dir() \u001B[38;5;129;01mand\u001B[39;00m category_dir\u001B[38;5;241m.\u001B[39mjoinpath(image_dir)\u001B[38;5;241m.\u001B[39mis_dir()):\n\u001B[0;32m     97\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\Lib\\pathlib.py:931\u001B[0m, in \u001B[0;36mPath.iterdir\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    927\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21miterdir\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    928\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001B[39;00m\n\u001B[0;32m    929\u001B[0m \u001B[38;5;124;03m    result for the special paths '.' and '..'.\u001B[39;00m\n\u001B[0;32m    930\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 931\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    932\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_child_relpath(name)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: '\\\\storage\\\\remote\\\\adm9\\\\cv3dst\\\\cv3ws23\\\\cv3_exercises\\\\cv3dst\\\\datasets\\\\obj_seg\\\\train'"
     ]
    }
   ],
   "source": [
    "INITIAL_RUN = True\n",
    "PATCH_SIZE = 16\n",
    "\n",
    "if INITIAL_RUN:\n",
    "    dino = torch.hub.load(\"facebookresearch/dino:main\", f\"dino_vits{PATCH_SIZE}\")\n",
    "\n",
    "    dino = dino.to(device)\n",
    "    dino.eval()\n",
    "\n",
    "    print(\"Creating embddings for the training set\")\n",
    "    create_embeddings(dataset_dir.joinpath(\"obj_seg\", \"train\"), dino, device)\n",
    "    print(\"Downsampling annotations for the training set\")\n",
    "    downsample_annotations(dataset_dir.joinpath(\"obj_seg\", \"train\"), PATCH_SIZE)\n",
    "    print(\"Creating embddings for the test set\")\n",
    "    create_embeddings(dataset_dir.joinpath(\"obj_seg\", \"test\"), dino, device)\n",
    "    print(\"Downsampling annotations for the test set\")\n",
    "    downsample_annotations(dataset_dir.joinpath(\"obj_seg\", \"test\"), PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some information about DINO ...\n",
    "\n",
    "DINO is short for self-**di**stillation with **no** labels. It was introduced at ICCV in 2021 [1]. It uses the paradigm of self-supervised learning to train with random, unlabeled examples. Thus, it can use large amounts of data compared to supervised methods that require annotations. The recent advances in vision transformers (ViTs) allow DINO to produce robust and reliable image representations in latent space/embeddings. These embeddings can be used for unsupervised tasks (see exercise 05) or leveraged for supervised tasks. As we will see, the pre-trained embeddings allow these supervised tasks to use only a fraction of the previously needed annotated data. Therefore, we can learn a meaningful segmentation net even with our limited computing power in this course.\n",
    "\n",
    "The following gives a quick rundown of the training scheme of the original DINO.\n",
    "\n",
    "## Knowledge Distillation\n",
    "Knowledge distillation aims for a student network $F_s(x)$ to reproduce the output of an already trained teacher network $F_t(x)$. The original intention was to take a large teacher network and a small student network to reduce computational demand at inference time [4]. The training supervision was done on the logit outputs of the teacher model to distil as much information as possible. \n",
    "\n",
    "## Self-Distillation\n",
    "Self-distillation adopts this scheme so that the student and teacher networks share the same architecture. DINO trains them simultaneously, employing the exponential moving average (EMA) to calculate the teacher weights $\\theta_t$ from the past student weights $\\theta_s$.\n",
    "$$\\theta_t \\gets \\lambda \\theta_t - (1 - \\lambda)\\theta_s$$\n",
    "In the training scheme of DINO, two crops of the same image $x$ are given to the student and teacher networks. A cross-entropy loss then compares the output distributions of both networks for the different crops. The output is an additional output token, the so-called [CLS] token added to the patch tokens, encoding the whole image.\n",
    "\n",
    "## Vision Transformer\n",
    "The ViT of DINO divides an image into patches of $16 \\times 16$ pixels (versions with $8 \\times 8$ pixels are also available) and encodes them with a linear layer to an embedding space. These patch tokens are concatenated with an initial [CLS] token and fed through transformer layers to produce a final set of embeddings (for each patch + cls). Those embeddings encode semantic information, and recent papers have used those for downstream tasks.\n",
    "\n",
    "For more information, we recommend checking out the paper [1] and the follow-up work of DINOv2 [2], which provides even better embeddings.\n",
    "If you want to gain insight into what the cls-token and patch-tokens are encoding, you can also have a look at [6], where unprocessed plain DINO features are solving for the texture transfer task.\n",
    "Another work [7] shows results on unsupervised clustering in the DINO feature space.\n",
    "The related work of [3] also provides an exciting insight into vision transformers [3].\n",
    "\n",
    "[1] Caron et al. Emerging Properties in Self-Supervised Vision Transformers, ICCV 2021\n",
    "[2] Oquab et al. DINOv2: Learning Robust Visual Features without Supervision, arXiv 2023\n",
    "[3] Darcet et al. Vision Transformers Need Registers, arXiv 2023\n",
    "[4] Hinton et al. Distilling the Knowledge in a Neural Network, arXiv 2015\n",
    "[5] Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recognition at scale, ICLR 2021\n",
    "[6] Tumanyan et al. Splicing ViT Features for Semantic Appearance Transfer\n",
    "[7] Amir et al. Deep ViT Features as Dense Visual Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Object Segmentation Datasets\n",
    "\n",
    "In this exercise, we will be working with a subset of the [DAVIS 2016](https://davischallenge.org/davis2016/browse.html) [1] dataset. DAVIS provides densely annotated video segmentation data. We use the 2016 DAVIS version with single instance annotation per video sequence.\n",
    "\n",
    "[1] Perazzi et al. A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation, CVPR 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T17:00:45.439104300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_davis_dataset(dataset_dir.joinpath(\"obj_seg\"), PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dataset\n",
    "\n",
    "In the following we provide a visualization of the dataset. To better understand the problem we visualize\n",
    "* the input images\n",
    "* the input image with overlaid annotations\n",
    "* the input image overlaid with annotations downsampled to the patch size\n",
    "* the embeddings per patch with the first 3 components of PCA mapped to RGB colors\n",
    "  \n",
    "You can visualize different examples by changing the index provided as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T17:00:45.442319400Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_davis(train_dataset, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Part I - Linear Probing\n",
    "\n",
    "In this exercise we will be using the simplest of network architectures to classify each patch as either foreground or background based on their DINO embeddings. This allows us to easily get a patch based image segmentation.\n",
    "\n",
    "Your task is to complete the simple network architecture that gets an patch embedding and outputs a binary classification score between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Linear Probing</h3>\n",
    "    <p> Go to <code>exercise_code/data/obj_segmentation/linear_probing.py</code> and the <code>LinearProbingNet</code> class. You need to implement a <b>one</b> layer network with a sigmoid layer by completing the <code>__init__</code> and <code>forward</code> methods of the network.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Test: Linear Probing</h3>\n",
    "    <p> Run the following cell to execute the test case for the linear probing.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T17:00:45.448738Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = test_linear_probing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous test ran successfully you can now train a linear probing network on pretrained DiNO patch embeddings to produce coarse object segmentation masks. The input dimension is set to 384 as this is the dimensionality of the DINO embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:00:45.476411400Z",
     "start_time": "2024-01-16T17:00:45.453754800Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_probing = LinearProbingNet(384)\n",
    "\n",
    "batch_size = 16\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    # \"test\": DataLoader(test_dataset, batch_size=batch_size, drop_last=False),\n",
    "    \"test\": DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False),\n",
    "}\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(linear_probing.parameters(), lr=5.0e-4)\n",
    "\n",
    "training(linear_probing, dataloaders, loss_fn, optimizer, 10, device)\n",
    "torch.save(linear_probing.state_dict(), output_dir.joinpath(\"linear_net.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results should be approximately. Metrics marked with <sup>*</sup> are calculated with regard to the non-downsampled ground truth.\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td>Class</td>\n",
    "            <td>Acc</td>\n",
    "            <td>Prcn</td>\n",
    "            <td>Rcll</td>\n",
    "            <td>IoU</td>\n",
    "            <td></td>\n",
    "            <td>Acc<sup>*</sup></td>\n",
    "            <td>Prcn<sup>*</sup></td>\n",
    "            <td>Rcll<sup>*</sup></td>\n",
    "            <td>IoU<sup>*</sup></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Bear</td>\n",
    "            <td>98%</td>\n",
    "            <td>0.94</td>\n",
    "            <td>0.96</td>\n",
    "            <td>0.90</td>\n",
    "            <td></td>\n",
    "            <td>97%</td>\n",
    "            <td>0.87</td>\n",
    "            <td>0.96</td>\n",
    "            <td>0.90</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Dog</td>\n",
    "            <td>94%</td>\n",
    "            <td>0.86</td>\n",
    "            <td>0.96</td>\n",
    "            <td>0.82</td>\n",
    "            <td></td>\n",
    "            <td>95%</td>\n",
    "            <td>0.83</td>\n",
    "            <td>0.89</td>\n",
    "            <td>0.92</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Overall</td>\n",
    "            <td>96%</td>\n",
    "            <td>0.90</td>\n",
    "            <td>0.96</td>\n",
    "            <td>0.86</td>\n",
    "            <td></td>\n",
    "            <td>96%</td>\n",
    "            <td>0.85</td>\n",
    "            <td>0.92</td>\n",
    "            <td>0.91</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results\n",
    "\n",
    "To get a feel for the performance of your model, you can visualize the image segmentations and compare them to the ground truth. The index for the dataset is passed as the third argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T17:00:45.469402100Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_model(linear_probing, train_dataset, 0, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Refining the Prediction with Pixel-Adaptive Convolutional Networks\n",
    "\n",
    "Linear probing gives us already good results for object segmentation. As the DiNO features provide powerful high-level embeddings of the patches. As the patches have as size of $16 \\times 16$ pixels the predictions are very coarse. Details of the object are not visible in the segmentation masks. We can also see this in the metrics calculated with the non-downsampled ground truth masks.\n",
    "\n",
    "To address the coarse predictions of the masks, we need to upsample these results. In this exercise we look at pixel-adaptive convolutional (PAC) layers introduced in [1] to do the upsampling. Unlike traditional upconvolution layers, PAC layers don't have fixed convolutional filters but rather filters that are adapted with the image content to produce spatially variant filters that give the network more flexibility.\n",
    "\n",
    "The spatially variant filters are produces by adapting the convolutional weights with a spacially varying kernel. The kernel weights are dependent on the local image features. See the exercise slides for an visual example.\n",
    "\n",
    "### Kernel\n",
    "In this exercise we use a gaussian kernel. Given an image patch the kernel weights for a pixel $i$ are calculated by\n",
    "$$K(\\boldsymbol{f}_i, \\boldsymbol{f}_j) = \\exp\\left(-\\frac{1}{2} (\\boldsymbol{f}_i - \\boldsymbol{f}_j)^\\top (\\boldsymbol{f}_i - \\boldsymbol{f}_j) \\right)\\,,$$\n",
    "where $j$ denotes the center pixel of the patch and $\\boldsymbol{f}_i, \\boldsymbol{f}_j$ are the respective feature vectors. In our case, the features are the pixels RGB values.\n",
    "\n",
    "[1] [Su et al., Pixel-Adaptive Convolutional Networks Networks, CVPR 2019](https://arxiv.org/pdf/1904.05373.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Pixel-Adaptive Convolutional Networks</h3>\n",
    "    <p> Go to <code>exercise_code/data/obj_segmentation/pixel_adaptive_conv_networks.py</code>. You need to complete the methods <code>packernel2d</code> for computing the kernel weights and <code>pacconv2d</code> for doing the pixel-adaptive convolution. Please refer to the exercise slides for visual examples on how to implement both functions.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Test: Pixel-Adaptive Convolutional Networks</h3>\n",
    "    <p> Run the following cell to execute the test case for the pac layers.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T17:00:45.470520200Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = test_pix_adaptive_conv_nets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous test ran successfully you can now train a linear probing network on pretrained DiNO patch embeddings to produce coarse object segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-16T17:00:45.473519200Z"
    }
   },
   "outputs": [],
   "source": [
    "pac_net = PACNet()\n",
    "pac_net.to(device)\n",
    "\n",
    "batch_size = 4\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    \"test\": DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False),\n",
    "}\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(pac_net.parameters(), lr=5.0e-4)\n",
    "\n",
    "training(pac_net, dataloaders, loss_fn, optimizer, 5, device)\n",
    "torch.save(pac_net.state_dict(), output_dir.joinpath(\"pac_net.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results should be approximately.\n",
    "<div>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td>Class</td>\n",
    "            <td>Acc</td>\n",
    "            <td>Prcn</td>\n",
    "            <td>Rcll</td>\n",
    "            <td>IoU</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Bear</td>\n",
    "            <td>99%</td>\n",
    "            <td>0.97</td>\n",
    "            <td>0.97</td>\n",
    "            <td>0.94</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Dog</td>\n",
    "            <td>98%</td>\n",
    "            <td>0.98</td>\n",
    "            <td>0.95</td>\n",
    "            <td>0.94</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Overall</td>\n",
    "            <td>99%</td>\n",
    "            <td>0.98</td>\n",
    "            <td>0.96</td>\n",
    "            <td>0.94</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Again, you can visualize the performance of your network and compare it to the ground truth. This time you will see that the predictions are on a per-pixel instead of a per-patch level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:00:45.478629200Z",
     "start_time": "2024-01-16T17:00:45.476411400Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_model(pac_net, train_dataset, 0, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:00:45.555256200Z",
     "start_time": "2024-01-16T17:00:45.479953700Z"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('../output/exercise04')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('cv3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b38045e10271186d31b9c7cfcf32f44b81f9b46f72bad763493647421023d2a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
